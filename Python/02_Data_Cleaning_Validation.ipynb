{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c906549e-d680-467e-ba24-c968745e2a65",
   "metadata": {},
   "source": [
    "# üßº 02 ‚Äì Data Cleaning & Validation\n",
    "\n",
    "This notebook is part of the **Sales Health Monitor** project.\n",
    "\n",
    "## üéØ Objective\n",
    "The goal of this notebook is to:\n",
    "1. **Assess** the quality of the raw (corrupted) sales dataset.\n",
    "2. **Detect** issues such as missing values, duplicates, outliers, and incorrect formats.\n",
    "3. **Clean** the dataset using generalized methods that can be reused in future projects.\n",
    "4. **Validate** the cleaned dataset to ensure it meets business and data quality standards.\n",
    "5. **Export** the cleaned dataset for further analysis in Python, MySQL, and Power BI.\n",
    "\n",
    "## üîÑ Why This Step Matters\n",
    "Data quality directly impacts business insights and decision-making.  \n",
    "By applying a systematic cleaning and validation process, we ensure that:\n",
    "- The analysis phase will run smoothly without errors.\n",
    "- The ML anomaly detection model will be trained on reliable data.\n",
    "- This cleaning framework can be reused for future datasets with minimal changes.\n",
    "\n",
    "---\n",
    "\n",
    "We will begin with **Step 1 ‚Äì Setup & Profiling**, which includes:\n",
    "- Importing required Python libraries.\n",
    "- Defining project configuration variables.\n",
    "- Loading the dataset.\n",
    "- Running initial profiling to understand its structure and potential issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1879d3-783c-489f-a372-6365f02901ac",
   "metadata": {},
   "source": [
    "## 1.1 Project Setup ‚Äì Imports & Configuration\n",
    "\n",
    "In this section:\n",
    "- Import Python libraries for data loading, profiling, and cleaning.\n",
    "- Define **configuration variables** such as file paths, column categories, and business rules.\n",
    "\n",
    "### Why This Matters\n",
    "By centralising settings here, this notebook becomes:\n",
    "- **Reusable**: Future datasets only require changes to this section.\n",
    "- **Easy to maintain**: No need to search through the notebook for hardcoded values.\n",
    "\n",
    "*(I prefer this approach so I can quickly adapt the cleaning process to new projects without rewriting the code.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "251d85e7-154f-4aad-89c9-8ebd6fac6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1.1 Import Required Libraries ====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== 1.2 Project Configuration ====\n",
    "PROJECT_NAME = \"Sales Health Monitor\"\n",
    "\n",
    "DATA_PATHS = {\n",
    "    \"raw\": \"../Dataset/raw/\",\n",
    "    \"processed\": \"../Dataset/processed/\",\n",
    "    \"sample\": \"../Dataset/sample/\"\n",
    "}\n",
    "\n",
    "DATASET_CONFIGS = {\n",
    "    \"main_dataset\": {\n",
    "        \"file\": \"sales_corrupted.csv\",  # File to clean\n",
    "        \"date_columns\": [\"transaction_date\"],  # Columns with date values\n",
    "        \"id_columns\": [\"transaction_id\", \"customer_id\"],  # Key ID columns\n",
    "        \"numeric_columns\": [\"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\"],\n",
    "        \"categorical_columns\": [\"customer_segment\", \"product_category\", \"region\", \"sales_channel\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "BUSINESS_RULES = {\n",
    "    \"date_range\": (\"2022-01-01\", \"2024-12-31\"),\n",
    "    \"quantity_range\": (0, 1000),\n",
    "    \"unit_price_range\": (0, 100000),\n",
    "    \"discount_percent_range\": (0, 100),\n",
    "    \"total_amount_range\": (0, 1000000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044509eb-0820-4a0a-9cd3-50c3cf9f3bc4",
   "metadata": {},
   "source": [
    "## 1.2 Load Dataset\n",
    "\n",
    "In this step:\n",
    "- Load the **raw corrupted dataset** defined in `DATASET_CONFIGS`.\n",
    "- Display basic information to confirm successful loading.\n",
    "- Preview the first few rows to understand the structure.\n",
    "\n",
    "### Why This Matters\n",
    "Before starting any profiling or cleaning:\n",
    "- We need to ensure the file path and name are correct.\n",
    "- A quick preview helps spot obvious problems (e.g., wrong delimiter, unexpected columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5c3a89f-ccca-4ddf-97c6-20231a678169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sales Health Monitor - Dataset Loaded Successfully\n",
      "üìä Shape: 801,440 rows √ó 13 columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>transaction_datetime</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>region</th>\n",
       "      <th>sales_channel</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>discount_percent</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_00000001</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-01-01 14:43:00</td>\n",
       "      <td>CUST_022237</td>\n",
       "      <td>Standard</td>\n",
       "      <td>PROD_0302</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>South</td>\n",
       "      <td>Mobile App</td>\n",
       "      <td>1</td>\n",
       "      <td>246.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_00000002</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-01-01 20:50:00</td>\n",
       "      <td>CUST_034632</td>\n",
       "      <td>Budget</td>\n",
       "      <td>PROD_0173</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>East</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>2</td>\n",
       "      <td>91.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_00000003</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-01-01 11:18:00</td>\n",
       "      <td>CUST_046496</td>\n",
       "      <td>Standard</td>\n",
       "      <td>PROD_0157</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>West</td>\n",
       "      <td>Online</td>\n",
       "      <td>1</td>\n",
       "      <td>144.35</td>\n",
       "      <td>9.0</td>\n",
       "      <td>144.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_00000004</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-01-01 07:52:00</td>\n",
       "      <td>CUST_003169</td>\n",
       "      <td>Budget</td>\n",
       "      <td>PROD_0139</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>East</td>\n",
       "      <td>Mobile App</td>\n",
       "      <td>1</td>\n",
       "      <td>165.72</td>\n",
       "      <td>7.6</td>\n",
       "      <td>165.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_00000005</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-01-01 09:33:00</td>\n",
       "      <td>CUST_045584</td>\n",
       "      <td>Standard</td>\n",
       "      <td>PROD_0138</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>West</td>\n",
       "      <td>Mobile App</td>\n",
       "      <td>2</td>\n",
       "      <td>72.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id transaction_date transaction_datetime  customer_id  \\\n",
       "0   TXN_00000001       2022-01-01  2022-01-01 14:43:00  CUST_022237   \n",
       "1   TXN_00000002       2022-01-01  2022-01-01 20:50:00  CUST_034632   \n",
       "2   TXN_00000003       2022-01-01  2022-01-01 11:18:00  CUST_046496   \n",
       "3   TXN_00000004       2022-01-01  2022-01-01 07:52:00  CUST_003169   \n",
       "4   TXN_00000005       2022-01-01  2022-01-01 09:33:00  CUST_045584   \n",
       "\n",
       "  customer_segment product_id   product_category region sales_channel  \\\n",
       "0         Standard  PROD_0302  Sports & Outdoors  South    Mobile App   \n",
       "1           Budget  PROD_0173           Clothing   East  Retail Store   \n",
       "2         Standard  PROD_0157           Clothing   West        Online   \n",
       "3           Budget  PROD_0139           Clothing   East    Mobile App   \n",
       "4         Standard  PROD_0138           Clothing   West    Mobile App   \n",
       "\n",
       "   quantity  unit_price  discount_percent  total_amount  \n",
       "0         1      246.29               0.0        246.29  \n",
       "1         2       91.23               0.0        182.46  \n",
       "2         1      144.35               9.0        144.35  \n",
       "3         1      165.72               7.6        165.72  \n",
       "4         2       72.16               0.0        144.32  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Load the dataset based on config ====\n",
    "\n",
    "# Build full path\n",
    "dataset_path = os.path.join(DATA_PATHS[\"raw\"], DATASET_CONFIGS[\"main_dataset\"][\"file\"])\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Show basic info\n",
    "print(f\"‚úÖ {PROJECT_NAME} - Dataset Loaded Successfully\")\n",
    "print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]:,} columns\\n\")\n",
    "\n",
    "# Saving a copy\n",
    "df_raw = df.copy()\n",
    "\n",
    "# Preview first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec34fdc-e1e7-4f55-9d04-97d59819e89a",
   "metadata": {},
   "source": [
    "## 1.3 Initial Profiling\n",
    "\n",
    "In this step:\n",
    "- Review the dataset‚Äôs structure and column data types.\n",
    "- Identify:\n",
    "  - Missing values per column.\n",
    "  - Duplicate rows.\n",
    "  - Summary statistics for numeric and categorical columns.\n",
    "\n",
    "### Why This Matters\n",
    "Initial profiling acts as a **data health check**:\n",
    "- Confirms that the dataset loaded correctly.\n",
    "- Highlights the scale and type of quality issues.\n",
    "- Provides a **baseline** for comparing before vs after cleaning results.\n",
    "\n",
    "*(I think of this step as the ‚ÄúX-ray‚Äù before surgery ‚Äî it tells us exactly where the problems are.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "204780f0-146d-40aa-9585-810ab4546f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 801440 entries, 0 to 801439\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   transaction_id        801440 non-null  object \n",
      " 1   transaction_date      801440 non-null  object \n",
      " 2   transaction_datetime  801440 non-null  object \n",
      " 3   customer_id           785399 non-null  object \n",
      " 4   customer_segment      801440 non-null  object \n",
      " 5   product_id            801440 non-null  object \n",
      " 6   product_category      801440 non-null  object \n",
      " 7   region                801440 non-null  object \n",
      " 8   sales_channel         801440 non-null  object \n",
      " 9   quantity              801440 non-null  int64  \n",
      " 10  unit_price            801440 non-null  float64\n",
      " 11  discount_percent      801440 non-null  float64\n",
      " 12  total_amount          801440 non-null  float64\n",
      "dtypes: float64(3), int64(1), object(9)\n",
      "memory usage: 79.5+ MB\n",
      "\n",
      "\n",
      "üîç Missing Values per Column:\n",
      "customer_id    16041\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "ü™û Duplicate Rows (exact matches): 0\n",
      "\n",
      "üìä Numeric Columns Summary:\n",
      "                     count        mean          std   min      25%     50%  \\\n",
      "quantity          801440.0    1.566489     1.160887 -9.00   1.0000    1.00   \n",
      "unit_price        801440.0  376.630203   469.912224  3.88  73.4775  200.32   \n",
      "discount_percent  801440.0    2.244919     5.799553  0.00   0.0000    0.00   \n",
      "total_amount      801440.0  658.881526  3498.591288  3.88  92.5500  261.55   \n",
      "\n",
      "                     75%           max  \n",
      "quantity            2.00  9.000000e+00  \n",
      "unit_price        428.82  2.229430e+03  \n",
      "discount_percent    0.00  2.500000e+01  \n",
      "total_amount      633.56  1.255612e+06  \n",
      "\n",
      "\n",
      "üìä Categorical Columns Unique Value Counts:\n",
      "customer_segment: 3 unique values\n",
      "product_category: 5 unique values\n",
      "region: 25 unique values\n",
      "sales_channel: 4 unique values\n"
     ]
    }
   ],
   "source": [
    "# ==== 1.3 Initial Profiling ====\n",
    "\n",
    "# --- Dataset Structure & Data Types ---\n",
    "print(\"üìã Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Missing Values ---\n",
    "print(\"üîç Missing Values per Column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "print(missing_counts if not missing_counts.empty else \"No missing values found.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Duplicate Rows (exact matches) ---\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"ü™û Duplicate Rows (exact matches): {duplicate_count}\\n\")\n",
    "\n",
    "# --- Numeric Summary ---\n",
    "numeric_cols = DATASET_CONFIGS[\"main_dataset\"][\"numeric_columns\"]\n",
    "print(\"üìä Numeric Columns Summary:\")\n",
    "print(df[numeric_cols].describe().T)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Categorical Summary ---\n",
    "categorical_cols = DATASET_CONFIGS[\"main_dataset\"][\"categorical_columns\"]\n",
    "print(\"üìä Categorical Columns Unique Value Counts:\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c97d40-8ec0-4d73-a9e3-1b0c1a685a93",
   "metadata": {},
   "source": [
    "## 1.4 Outlier Preview\n",
    "\n",
    "In this step:\n",
    "- Identify potential extreme values in numeric columns using the **Interquartile Range (IQR)** method.\n",
    "- This is only an **early detection** step ‚Äî we are not removing or adjusting outliers yet.\n",
    "\n",
    "### Why This Matters\n",
    "- Confirms whether extreme values seen in summary stats are genuine anomalies.\n",
    "- Helps guide later cleaning decisions (remove, cap, or keep).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9ab9526-02bf-4492-b788-cacc8881d3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity: 24,860 potential outliers\n",
      "unit_price: 86,883 potential outliers\n",
      "discount_percent: 119,878 potential outliers\n",
      "total_amount: 89,771 potential outliers\n"
     ]
    }
   ],
   "source": [
    "# ==== 1.4 Outlier Preview using IQR Method ====\n",
    "\n",
    "numeric_cols = DATASET_CONFIGS[\"main_dataset\"][\"numeric_columns\"]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outlier_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "    print(f\"{col}: {outlier_count:,} potential outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb4c57-3808-4b31-9fd7-3b3127e5b3b4",
   "metadata": {},
   "source": [
    "# Step 2 ‚Äì Cleaning Pipeline\n",
    "\n",
    "In this step:\n",
    "- Apply data cleaning techniques to address the issues found during Step 1 profiling.\n",
    "- Focus on:\n",
    "  1. Missing values.\n",
    "  2. Out-of-range or invalid numeric values.\n",
    "  3. Inconsistent categorical values.\n",
    "  4. Duplicate records.\n",
    "\n",
    "The goal is to ensure that the dataset:\n",
    "- Meets the defined business rules.\n",
    "- Is consistent and free from obvious quality issues.\n",
    "- Is ready for deeper analysis in later stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023e1a8-effd-4fee-81eb-0eadd48eea12",
   "metadata": {},
   "source": [
    "## 2.1 Handling Missing Values\n",
    "\n",
    "Missing values can occur for many reasons:\n",
    "- Data entry errors\n",
    "- System glitches\n",
    "- Deliberate masking of unavailable information\n",
    "\n",
    "### Approach in This Framework\n",
    "1. **ID columns** (`customer_id`):\n",
    "   - If missing ‚Üí drop those rows (IDs are essential for tracking transactions).\n",
    "2. **Categorical columns**:\n",
    "   - If missing ‚Üí replace with `\"Unknown\"`.\n",
    "3. **Numeric columns**:\n",
    "   - If missing ‚Üí replace with median value (less sensitive to outliers than mean).\n",
    "\n",
    "This function is flexible:\n",
    "- It reads column roles from `DATASET_CONFIGS`.\n",
    "- It allows quick changes in future projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c5ad83f-b597-4d62-84c6-c61abf5e1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Missing Values BEFORE Cleaning:\n",
      "customer_id    16041\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "üóë Dropped 16,041 rows with missing customer_id\n",
      "--------------------------------------------------\n",
      "‚úÖ Missing Values AFTER Cleaning:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "def handle_missing_values(df, configs):\n",
    "    \"\"\"\n",
    "    Handles missing values for numeric, categorical, and ID columns\n",
    "    based on the dataset configuration.\n",
    "    Prints before/after missing value counts.\n",
    "    \"\"\"\n",
    "    dataset_conf = configs[\"main_dataset\"]\n",
    "    \n",
    "    print(\"üîç Missing Values BEFORE Cleaning:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Drop rows where ID is missing\n",
    "    for id_col in dataset_conf[\"id_columns\"]:\n",
    "        before_rows = df.shape[0]\n",
    "        df = df[df[id_col].notna()]\n",
    "        after_rows = df.shape[0]\n",
    "        dropped = before_rows - after_rows\n",
    "        if dropped > 0:\n",
    "            print(f\"üóë Dropped {dropped:,} rows with missing {id_col}\")\n",
    "    \n",
    "    # Fill missing numeric values with median\n",
    "    for col in dataset_conf[\"numeric_columns\"]:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            print(f\"üßÆ Filled missing '{col}' with median = {median_val}\")\n",
    "    \n",
    "    # Fill missing categorical values with 'Unknown'\n",
    "    for col in dataset_conf[\"categorical_columns\"]:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(\"Unknown\", inplace=True)\n",
    "            print(f\"üî§ Filled missing '{col}' with 'Unknown'\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"‚úÖ Missing Values AFTER Cleaning:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply missing value handling\n",
    "df = handle_missing_values(df, DATASET_CONFIGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7db31c-2839-4f5d-8389-d9d4ecf44085",
   "metadata": {},
   "source": [
    "## 2.2 Handling Invalid Values\n",
    "\n",
    "In this step:\n",
    "- Enforce **BUSINESS_RULES** for numeric columns.\n",
    "- Identify and fix:\n",
    "  1. Negative or zero quantities.\n",
    "  2. Unrealistic unit prices.\n",
    "  3. Discounts outside the allowed range.\n",
    "  4. Total amounts outside realistic bounds.\n",
    "\n",
    "### Approach:\n",
    "- For each numeric column, compare values to the min/max from `BUSINESS_RULES`.\n",
    "- If values are **outside** the range:\n",
    "  - Quantities ‚Üí replace with median valid quantity.\n",
    "  - Prices & totals ‚Üí replace with median valid value.\n",
    "  - Discount ‚Üí cap at valid range (0‚Äì25% for this dataset, but can be configured).\n",
    "- Keep a **log** of how many rows were adjusted.\n",
    "\n",
    "This ensures:\n",
    "- Data stays realistic.\n",
    "- Extreme corrupted valu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa9f7485-1921-4c0d-930b-a4ab7a7f1ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Replaced 784 invalid 'quantity' values with median = 1.0\n",
      "‚úÖ No invalid values found in 'unit_price'\n",
      "‚úÖ No invalid values found in 'discount_percent'\n",
      "üîß Replaced 1 invalid 'total_amount' values with median = 261.55\n"
     ]
    }
   ],
   "source": [
    "def handle_invalid_values(df, configs, rules):\n",
    "    \"\"\"\n",
    "    Fixes numeric values that fall outside the acceptable ranges\n",
    "    defined in BUSINESS_RULES.\n",
    "    \"\"\"\n",
    "    dataset_conf = configs[\"main_dataset\"]\n",
    "    \n",
    "    for col in dataset_conf[\"numeric_columns\"]:\n",
    "        min_val, max_val = rules[f\"{col}_range\"] if f\"{col}_range\" in rules else (None, None)\n",
    "        \n",
    "        if min_val is not None and max_val is not None:\n",
    "            # Find invalid rows\n",
    "            invalid_mask = (df[col] < min_val) | (df[col] > max_val)\n",
    "            invalid_count = invalid_mask.sum()\n",
    "            \n",
    "            if invalid_count > 0:\n",
    "                if col == \"discount_percent\":\n",
    "                    # Cap discounts at limits\n",
    "                    df[col] = df[col].clip(lower=min_val, upper=max_val)\n",
    "                    print(f\"üîß Capped {invalid_count:,} values in '{col}' to range {min_val}‚Äì{max_val}\")\n",
    "                else:\n",
    "                    # Replace invalid numeric values with median of valid ones\n",
    "                    median_val = df.loc[~invalid_mask, col].median()\n",
    "                    df.loc[invalid_mask, col] = median_val\n",
    "                    print(f\"üîß Replaced {invalid_count:,} invalid '{col}' values with median = {median_val}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ No invalid values found in '{col}'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply invalid value handling\n",
    "df = handle_invalid_values(df, DATASET_CONFIGS, BUSINESS_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3ee8a75-56f1-4132-926e-af1aa109918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Numeric Summary (Before Cleaning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_acc72\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_acc72_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_acc72_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
       "      <th id=\"T_acc72_level0_col2\" class=\"col_heading level0 col2\" >std</th>\n",
       "      <th id=\"T_acc72_level0_col3\" class=\"col_heading level0 col3\" >min</th>\n",
       "      <th id=\"T_acc72_level0_col4\" class=\"col_heading level0 col4\" >25%</th>\n",
       "      <th id=\"T_acc72_level0_col5\" class=\"col_heading level0 col5\" >50%</th>\n",
       "      <th id=\"T_acc72_level0_col6\" class=\"col_heading level0 col6\" >75%</th>\n",
       "      <th id=\"T_acc72_level0_col7\" class=\"col_heading level0 col7\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_acc72_level0_row0\" class=\"row_heading level0 row0\" >quantity</th>\n",
       "      <td id=\"T_acc72_row0_col0\" class=\"data row0 col0\" >801440.00</td>\n",
       "      <td id=\"T_acc72_row0_col1\" class=\"data row0 col1\" >1.57</td>\n",
       "      <td id=\"T_acc72_row0_col2\" class=\"data row0 col2\" >1.16</td>\n",
       "      <td id=\"T_acc72_row0_col3\" class=\"data row0 col3\" >-9.00</td>\n",
       "      <td id=\"T_acc72_row0_col4\" class=\"data row0 col4\" >1.00</td>\n",
       "      <td id=\"T_acc72_row0_col5\" class=\"data row0 col5\" >1.00</td>\n",
       "      <td id=\"T_acc72_row0_col6\" class=\"data row0 col6\" >2.00</td>\n",
       "      <td id=\"T_acc72_row0_col7\" class=\"data row0 col7\" >9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_acc72_level0_row1\" class=\"row_heading level0 row1\" >unit_price</th>\n",
       "      <td id=\"T_acc72_row1_col0\" class=\"data row1 col0\" >801440.00</td>\n",
       "      <td id=\"T_acc72_row1_col1\" class=\"data row1 col1\" >376.63</td>\n",
       "      <td id=\"T_acc72_row1_col2\" class=\"data row1 col2\" >469.91</td>\n",
       "      <td id=\"T_acc72_row1_col3\" class=\"data row1 col3\" >3.88</td>\n",
       "      <td id=\"T_acc72_row1_col4\" class=\"data row1 col4\" >73.48</td>\n",
       "      <td id=\"T_acc72_row1_col5\" class=\"data row1 col5\" >200.32</td>\n",
       "      <td id=\"T_acc72_row1_col6\" class=\"data row1 col6\" >428.82</td>\n",
       "      <td id=\"T_acc72_row1_col7\" class=\"data row1 col7\" >2229.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_acc72_level0_row2\" class=\"row_heading level0 row2\" >discount_percent</th>\n",
       "      <td id=\"T_acc72_row2_col0\" class=\"data row2 col0\" >801440.00</td>\n",
       "      <td id=\"T_acc72_row2_col1\" class=\"data row2 col1\" >2.24</td>\n",
       "      <td id=\"T_acc72_row2_col2\" class=\"data row2 col2\" >5.80</td>\n",
       "      <td id=\"T_acc72_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
       "      <td id=\"T_acc72_row2_col4\" class=\"data row2 col4\" >0.00</td>\n",
       "      <td id=\"T_acc72_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
       "      <td id=\"T_acc72_row2_col6\" class=\"data row2 col6\" >0.00</td>\n",
       "      <td id=\"T_acc72_row2_col7\" class=\"data row2 col7\" >25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_acc72_level0_row3\" class=\"row_heading level0 row3\" >total_amount</th>\n",
       "      <td id=\"T_acc72_row3_col0\" class=\"data row3 col0\" >801440.00</td>\n",
       "      <td id=\"T_acc72_row3_col1\" class=\"data row3 col1\" >658.88</td>\n",
       "      <td id=\"T_acc72_row3_col2\" class=\"data row3 col2\" >3498.59</td>\n",
       "      <td id=\"T_acc72_row3_col3\" class=\"data row3 col3\" >3.88</td>\n",
       "      <td id=\"T_acc72_row3_col4\" class=\"data row3 col4\" >92.55</td>\n",
       "      <td id=\"T_acc72_row3_col5\" class=\"data row3 col5\" >261.55</td>\n",
       "      <td id=\"T_acc72_row3_col6\" class=\"data row3 col6\" >633.56</td>\n",
       "      <td id=\"T_acc72_row3_col7\" class=\"data row3 col7\" >1255612.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2057a8d3110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Numeric Summary (After Cleaning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ceb31\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ceb31_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_ceb31_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
       "      <th id=\"T_ceb31_level0_col2\" class=\"col_heading level0 col2\" >std</th>\n",
       "      <th id=\"T_ceb31_level0_col3\" class=\"col_heading level0 col3\" >min</th>\n",
       "      <th id=\"T_ceb31_level0_col4\" class=\"col_heading level0 col4\" >25%</th>\n",
       "      <th id=\"T_ceb31_level0_col5\" class=\"col_heading level0 col5\" >50%</th>\n",
       "      <th id=\"T_ceb31_level0_col6\" class=\"col_heading level0 col6\" >75%</th>\n",
       "      <th id=\"T_ceb31_level0_col7\" class=\"col_heading level0 col7\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ceb31_level0_row0\" class=\"row_heading level0 row0\" >quantity</th>\n",
       "      <td id=\"T_ceb31_row0_col0\" class=\"data row0 col0\" >785399.00</td>\n",
       "      <td id=\"T_ceb31_row0_col1\" class=\"data row0 col1\" >1.57</td>\n",
       "      <td id=\"T_ceb31_row0_col2\" class=\"data row0 col2\" >1.16</td>\n",
       "      <td id=\"T_ceb31_row0_col3\" class=\"data row0 col3\" >1.00</td>\n",
       "      <td id=\"T_ceb31_row0_col4\" class=\"data row0 col4\" >1.00</td>\n",
       "      <td id=\"T_ceb31_row0_col5\" class=\"data row0 col5\" >1.00</td>\n",
       "      <td id=\"T_ceb31_row0_col6\" class=\"data row0 col6\" >2.00</td>\n",
       "      <td id=\"T_ceb31_row0_col7\" class=\"data row0 col7\" >9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ceb31_level0_row1\" class=\"row_heading level0 row1\" >unit_price</th>\n",
       "      <td id=\"T_ceb31_row1_col0\" class=\"data row1 col0\" >785399.00</td>\n",
       "      <td id=\"T_ceb31_row1_col1\" class=\"data row1 col1\" >376.64</td>\n",
       "      <td id=\"T_ceb31_row1_col2\" class=\"data row1 col2\" >469.89</td>\n",
       "      <td id=\"T_ceb31_row1_col3\" class=\"data row1 col3\" >3.88</td>\n",
       "      <td id=\"T_ceb31_row1_col4\" class=\"data row1 col4\" >73.50</td>\n",
       "      <td id=\"T_ceb31_row1_col5\" class=\"data row1 col5\" >200.32</td>\n",
       "      <td id=\"T_ceb31_row1_col6\" class=\"data row1 col6\" >428.82</td>\n",
       "      <td id=\"T_ceb31_row1_col7\" class=\"data row1 col7\" >2229.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ceb31_level0_row2\" class=\"row_heading level0 row2\" >discount_percent</th>\n",
       "      <td id=\"T_ceb31_row2_col0\" class=\"data row2 col0\" >785399.00</td>\n",
       "      <td id=\"T_ceb31_row2_col1\" class=\"data row2 col1\" >2.25</td>\n",
       "      <td id=\"T_ceb31_row2_col2\" class=\"data row2 col2\" >5.80</td>\n",
       "      <td id=\"T_ceb31_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
       "      <td id=\"T_ceb31_row2_col4\" class=\"data row2 col4\" >0.00</td>\n",
       "      <td id=\"T_ceb31_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
       "      <td id=\"T_ceb31_row2_col6\" class=\"data row2 col6\" >0.00</td>\n",
       "      <td id=\"T_ceb31_row2_col7\" class=\"data row2 col7\" >25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ceb31_level0_row3\" class=\"row_heading level0 row3\" >total_amount</th>\n",
       "      <td id=\"T_ceb31_row3_col0\" class=\"data row3 col0\" >785399.00</td>\n",
       "      <td id=\"T_ceb31_row3_col1\" class=\"data row3 col1\" >657.41</td>\n",
       "      <td id=\"T_ceb31_row3_col2\" class=\"data row3 col2\" >3217.06</td>\n",
       "      <td id=\"T_ceb31_row3_col3\" class=\"data row3 col3\" >3.88</td>\n",
       "      <td id=\"T_ceb31_row3_col4\" class=\"data row3 col4\" >92.55</td>\n",
       "      <td id=\"T_ceb31_row3_col5\" class=\"data row3 col5\" >261.55</td>\n",
       "      <td id=\"T_ceb31_row3_col6\" class=\"data row3 col6\" >633.56</td>\n",
       "      <td id=\"T_ceb31_row3_col7\" class=\"data row3 col7\" >549722.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2057a8d3110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numeric_summary(df, label):\n",
    "    print(f\"\\nüìä Numeric Summary ({label})\")\n",
    "    display(\n",
    "        df[DATASET_CONFIGS[\"main_dataset\"][\"numeric_columns\"]]\n",
    "        .describe()\n",
    "        .T\n",
    "        .style.format(precision=2)\n",
    "    )\n",
    "\n",
    "# Show before cleaning\n",
    "numeric_summary(df_raw, \"Before Cleaning\")\n",
    "\n",
    "# Show after cleaning\n",
    "numeric_summary(df, \"After Cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c0a8d-233d-4142-97f3-d41f0812e936",
   "metadata": {},
   "source": [
    "## 2.3 Standardising Categorical Values\n",
    "\n",
    "In this step, I will standardise the categorical columns to ensure consistency across the dataset.  \n",
    "Due to data entry variations or intentional corruptions, the same category might appear in multiple forms (e.g., `\"North\"`, `\"N\"`, `\"Northern\"`).  \n",
    "I will focus on the following columns:  \n",
    "- `customer_segment`  \n",
    "- `product_category`  \n",
    "- `region`  \n",
    "- `sales_channel`  \n",
    "\n",
    "The `region` column will be cleaned using a mapping dictionary derived from the original business scenario. This will reduce the number of unique region values from 25 to the expected 5 standard values:  \n",
    "**North, South, East, West, Central**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f1bd44c-e219-483f-ba51-613cecce84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Column 'customer_segment': 3 ‚Üí 3 unique values after standardisation\n",
      "‚úÖ customer_segment values match expected set.\n",
      "üîß Column 'product_category': 5 ‚Üí 5 unique values after standardisation\n",
      "‚úÖ product_category values match expected set.\n",
      "üîß Column 'region': 5 ‚Üí 5 unique values after standardisation\n",
      "‚úÖ region values match expected set.\n",
      "üîß Column 'sales_channel': 4 ‚Üí 4 unique values after standardisation\n",
      "‚úÖ sales_channel values match expected set.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2.3: Standardising Categorical Values ---\n",
    "\n",
    "# Expected values dictionary (from business scenario)\n",
    "EXPECTED_CATEGORIES = {\n",
    "    \"customer_segment\": {\"Budget\", \"Premium\", \"Standard\"},\n",
    "    \"product_category\": {\"Electronics\", \"Clothing\", \"Home & Garden\", \"Sports & Outdoors\", \"Books & Media\"},\n",
    "    \"region\": {\"North\", \"South\", \"East\", \"West\", \"Central\"},\n",
    "    \"sales_channel\": {\"Online\", \"Retail Store\", \"Phone Orders\", \"Mobile App\"}\n",
    "}\n",
    "\n",
    "# Mapping dictionary for region standardisation\n",
    "REGION_MAPPING = {\n",
    "    # Central\n",
    "    \"c\": \"Central\", \"central\": \"Central\", \"centre\": \"Central\",\n",
    "    # East\n",
    "    \"e\": \"East\", \"east\": \"East\", \"eastern\": \"East\",\n",
    "    # North\n",
    "    \"n\": \"North\", \"north\": \"North\", \"northern\": \"North\",\n",
    "    # South\n",
    "    \"s\": \"South\", \"south\": \"South\", \"southern\": \"South\",\n",
    "    # West\n",
    "    \"w\": \"West\", \"west\": \"West\", \"western\": \"West\"\n",
    "}\n",
    "\n",
    "# Apply standardisation\n",
    "df[\"region\"] = df[\"region\"].str.strip().str.lower().map(REGION_MAPPING)\n",
    "\n",
    "# Store unique values before cleaning\n",
    "unique_before = {col: df[col].nunique() for col in DATASET_CONFIGS[\"main_dataset\"][\"categorical_columns\"]}\n",
    "\n",
    "# Apply standardisation\n",
    "df['region'] = df['region'].map(REGION_MAPPING).fillna(df['region'])\n",
    "\n",
    "# Store unique values after cleaning\n",
    "unique_after = {col: df[col].nunique() for col in DATASET_CONFIGS[\"main_dataset\"][\"categorical_columns\"]}\n",
    "\n",
    "# Logging changes\n",
    "for col in DATASET_CONFIGS[\"main_dataset\"][\"categorical_columns\"]:\n",
    "    print(f\"üîß Column '{col}': {unique_before[col]} ‚Üí {unique_after[col]} unique values after standardisation\")\n",
    "    \n",
    "    # --- Validation for each categorical column ---\n",
    "    actual_values = set(df[col].unique())\n",
    "    if actual_values != EXPECTED_CATEGORIES[col]:\n",
    "        print(f\"‚ö†Ô∏è Warning: {col} values do not match expected set.\\nFound: {actual_values}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {col} values match expected set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9013557-f290-4875-aa1a-af6f37ab4856",
   "metadata": {},
   "source": [
    "## 2.4 ‚Äì Statistical Outlier Detection (IQR Method)\n",
    "\n",
    "In this step, we identify potential statistical outliers in our numeric columns using the Interquartile Range (IQR) method.  \n",
    "Unlike business rule validation, which is based on predefined acceptable ranges, the IQR method looks for values that lie significantly outside the typical spread of the data.  \n",
    "\n",
    "- **Why keep outliers?**  \n",
    "  In many business datasets, extreme values may be valid (e.g., unusually high purchases during sales events).  \n",
    "  For this project, we will **detect but not modify** these values to preserve the original distribution for further analysis.\n",
    "\n",
    "**Steps:**\n",
    "1. For each numeric column:\n",
    "   - Calculate Q1 (25th percentile) and Q3 (75th percentile).\n",
    "   - Compute IQR = Q3 - Q1.\n",
    "   - Define outlier bounds as:  \n",
    "     - Lower bound = Q1 - 1.5 √ó IQR  \n",
    "     - Upper bound = Q3 + 1.5 √ó IQR  \n",
    "   - Count how many values fall outside this range.\n",
    "2. Display summary statistics before and after detection (no changes applied).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af5073a3-178c-4326-9cbd-c662db499aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No invalid values found in 'quantity'\n",
      "üìä Statistical outliers detected in 'quantity': 23579 values (not modified)\n",
      "--------------------------------------------------\n",
      "‚úÖ No invalid values found in 'unit_price'\n",
      "üìä Statistical outliers detected in 'unit_price': 85146 values (not modified)\n",
      "--------------------------------------------------\n",
      "‚úÖ No invalid values found in 'discount_percent'\n",
      "üìä Statistical outliers detected in 'discount_percent': 117485 values (not modified)\n",
      "--------------------------------------------------\n",
      "‚úÖ No invalid values found in 'total_amount'\n",
      "üìä Statistical outliers detected in 'total_amount': 87989 values (not modified)\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä Numeric Summary (Before & After - same since we didn't modify data)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>discount_percent</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>785399.000000</td>\n",
       "      <td>785399.000000</td>\n",
       "      <td>785399.000000</td>\n",
       "      <td>785399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.569160</td>\n",
       "      <td>376.636532</td>\n",
       "      <td>2.245034</td>\n",
       "      <td>657.410641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.156608</td>\n",
       "      <td>469.890756</td>\n",
       "      <td>5.799624</td>\n",
       "      <td>3217.058498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>261.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>428.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>633.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>2229.430000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>549722.319745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            quantity     unit_price  discount_percent   total_amount\n",
       "count  785399.000000  785399.000000     785399.000000  785399.000000\n",
       "mean        1.569160     376.636532          2.245034     657.410641\n",
       "std         1.156608     469.890756          5.799624    3217.058498\n",
       "min         1.000000       3.880000          0.000000       3.880000\n",
       "25%         1.000000      73.500000          0.000000      92.550000\n",
       "50%         1.000000     200.320000          0.000000     261.550000\n",
       "75%         2.000000     428.820000          0.000000     633.560000\n",
       "max         9.000000    2229.430000         25.000000  549722.319745"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def detect_statistical_outliers(df, numeric_columns):\n",
    "    \"\"\"\n",
    "    Detects statistical outliers in numeric columns using the IQR method.\n",
    "    Outliers are NOT removed or modified ‚Äî only counted for reference.\n",
    "    \"\"\"\n",
    "    for col in numeric_columns:\n",
    "        # Calculate Q1, Q3 and IQR\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Detect outliers\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        count_outliers = outliers.shape[0]\n",
    "\n",
    "        print(f\"‚úÖ No invalid values found in '{col}'\")\n",
    "        print(f\"üìä Statistical outliers detected in '{col}': {count_outliers} values (not modified)\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run outlier detection\n",
    "detect_statistical_outliers(\n",
    "    df,\n",
    "    DATASET_CONFIGS[\"main_dataset\"][\"numeric_columns\"]\n",
    ")\n",
    "\n",
    "# Show numeric summary before/after (no change expected)\n",
    "print(\"\\nüìä Numeric Summary (Before & After - same since we didn't modify data)\")\n",
    "display(df[DATASET_CONFIGS[\"main_dataset\"][\"numeric_columns\"]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac055cb-7fdd-41f9-92b3-b97524a34078",
   "metadata": {},
   "source": [
    "## 2.5 Date Parsing & Validation\n",
    "\n",
    "In this step, I will **parse and validate all date columns** in the dataset.  \n",
    "The aim is to ensure that:  \n",
    "\n",
    "1. All date values are converted into proper `datetime` objects.  \n",
    "2. Any invalid date formats or parsing errors are detected and removed.  \n",
    "3. All dates fall within the **business rule date range**:  \n",
    "   **`2020-01-01` to `2024-12-31`**.  \n",
    "\n",
    "For each date column, I will:  \n",
    "- Convert it to `datetime` (invalid entries become `NaT`).  \n",
    "- Count and display the number of invalid dates.  \n",
    "- Identify dates outside the defined range.  \n",
    "- Remove all invalid or out-of-range entries.  \n",
    "\n",
    "This ensures that **time-based analysis** in later steps will be accurate and free from corrupted date values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7cd1d75-b630-4356-b2f9-149b5f3f3b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóì Processing date column: 'transaction_date'\n",
      "‚ö†Ô∏è Found 391 out-of-range dates in 'transaction_date'\n",
      "üóë Removed 391 rows from 'transaction_date' after validation\n",
      "\n",
      "‚úÖ Date columns validated and parsed successfully\n"
     ]
    }
   ],
   "source": [
    "# 2.5 Date Validation & Cleaning\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def validate_and_clean_dates(df, date_columns, rules):\n",
    "    \"\"\"\n",
    "    Validates and cleans date columns based on business rules.\n",
    "    - Removes rows with out-of-range dates\n",
    "    - Converts to datetime format\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        print(f\"\\nüóì Processing date column: '{col}'\")\n",
    "        \n",
    "        # Convert to datetime safely\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "        # Drop NaT values\n",
    "        before_count = len(df)\n",
    "        df = df.dropna(subset=[col])\n",
    "        after_count = len(df)\n",
    "        dropped_nulls = before_count - after_count\n",
    "        if dropped_nulls > 0:\n",
    "            print(f\"üóë Removed {dropped_nulls} rows with invalid {col}\")\n",
    "\n",
    "        # Apply range filtering\n",
    "        min_date, max_date = pd.to_datetime(rules[\"date_range\"])\n",
    "        out_of_range_mask = (df[col] < min_date) | (df[col] > max_date)\n",
    "        out_of_range_count = out_of_range_mask.sum()\n",
    "        \n",
    "        if out_of_range_count > 0:\n",
    "            df = df[~out_of_range_mask]\n",
    "            print(f\"‚ö†Ô∏è Found {out_of_range_count} out-of-range dates in '{col}'\")\n",
    "            print(f\"üóë Removed {out_of_range_count} rows from '{col}' after validation\")\n",
    "\n",
    "    print(\"\\n‚úÖ Date columns validated and parsed successfully\")\n",
    "    return df\n",
    "\n",
    "# Apply function\n",
    "df = validate_and_clean_dates(\n",
    "    df,\n",
    "    DATASET_CONFIGS[\"main_dataset\"][\"date_columns\"],\n",
    "    BUSINESS_RULES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762594a-5695-46bc-b967-d46061a8cba2",
   "metadata": {},
   "source": [
    "## 2.6 Duplicate Detection & Removal\n",
    "\n",
    "Duplicate records can occur due to **system glitches**, **manual errors**, or **data integration issues**.  \n",
    "If not removed, they can **inflate totals**, **skew analysis**, and **mislead business decisions**.\n",
    "\n",
    "In this step, we handle two types of duplicates:\n",
    "\n",
    "1. **Exact duplicates** ‚Äì Rows that are identical across all columns.\n",
    "2. **Soft duplicates** ‚Äì Rows that are identical across all columns **except unique identifiers** (e.g., `transaction_id`).  \n",
    "   These often occur when the same transaction is recorded multiple times with a different ID suffix, as in our dataset (`_DUP` entries from the corruption process).\n",
    "\n",
    "**Approach:**\n",
    "- First, detect and remove **exact duplicates**.\n",
    "- Then, detect and remove **soft duplicates** by ignoring ID columns during comparison.\n",
    "- Report the number of duplicates found and removed.\n",
    "- Confirm the final row count after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86589fb5-7a94-4b29-b2dd-edacf154f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóë Removed 0 exact duplicate rows\n",
      "üóë Removed 7,720 soft duplicate rows (ignoring IDs)\n",
      "‚úÖ Total duplicates removed: 7,720\n",
      "üìä Final dataset size: 777,288 rows\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2.6 Duplicate Detection & Removal\n",
    "# ==========================================\n",
    "\n",
    "def remove_duplicates(df, id_columns):\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # --- Step 1: Remove exact duplicates ---\n",
    "    exact_dupes = df.duplicated()\n",
    "    exact_dupe_count = exact_dupes.sum()\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(f\"üóë Removed {exact_dupe_count:,} exact duplicate rows\")\n",
    "    \n",
    "    # --- Step 2: Remove soft duplicates (ignore ID columns) ---\n",
    "    non_id_cols = [col for col in df.columns if col not in id_columns]\n",
    "    soft_dupes = df.duplicated(subset=non_id_cols)\n",
    "    soft_dupe_count = soft_dupes.sum()\n",
    "    df = df.drop_duplicates(subset=non_id_cols)\n",
    "    \n",
    "    print(f\"üóë Removed {soft_dupe_count:,} soft duplicate rows (ignoring IDs)\")\n",
    "    \n",
    "    # --- Final dataset size ---\n",
    "    final_count = len(df)\n",
    "    removed_total = initial_count - final_count\n",
    "    print(f\"‚úÖ Total duplicates removed: {removed_total:,}\")\n",
    "    print(f\"üìä Final dataset size: {final_count:,} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply duplicate removal\n",
    "df = remove_duplicates(df, DATASET_CONFIGS[\"main_dataset\"][\"id_columns\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07016630-43f6-46ac-a6e9-b0fe166e00a9",
   "metadata": {},
   "source": [
    "## 2.7 ‚Äì Export Final Clean Dataset\n",
    "\n",
    "After applying all data cleaning steps ‚Äî including handling missing values, correcting invalid entries, standardising categorical values, validating dates, and removing duplicates ‚Äî the dataset is now ready for analysis and integration into other tools.\n",
    "\n",
    "Exporting the cleaned dataset to a CSV file ensures:\n",
    "\n",
    "- **Reusability**: Can be imported into BI dashboards, SQL databases, or ML pipelines without re-running the cleaning script.\n",
    "- **Traceability**: Acts as a permanent record of the cleaned version for reproducibility.\n",
    "- **Performance**: Avoids repeating time-consuming cleaning steps in future analyses.\n",
    "\n",
    "We will now save the final DataFrame to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ffc0a7a-5dd6-4555-88b1-bf9f65b917f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã COMPREHENSIVE VALIDATION\n",
      "========================================\n",
      "   üìÖ Date range: 2022-01-01 to 2024-12-31 ‚úÖ\n",
      "   üì¶ Quantity range: 1 to 9 ‚úÖ\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'amount_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m amt_min = df[\u001b[33m'\u001b[39m\u001b[33mtotal_amount\u001b[39m\u001b[33m'\u001b[39m].min()\n\u001b[32m     27\u001b[39m amt_max = df[\u001b[33m'\u001b[39m\u001b[33mtotal_amount\u001b[39m\u001b[33m'\u001b[39m].max()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m amt_valid = (amt_min >= \u001b[43mBUSINESS_RULES\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamount_range\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m (amt_max <= BUSINESS_RULES[\u001b[33m\"\u001b[39m\u001b[33mamount_range\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m1\u001b[39m])\n\u001b[32m     29\u001b[39m validation_results[\u001b[33m'\u001b[39m\u001b[33mamount_valid\u001b[39m\u001b[33m'\u001b[39m] = amt_valid\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   üí∞ Amount range: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mamt_min\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mamt_max\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m‚úÖ\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mamt_valid\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m‚ùå\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'amount_range'"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE VALIDATION\n",
    "print(f\"\\nüìã COMPREHENSIVE VALIDATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check all business rules\n",
    "validation_results = {}\n",
    "\n",
    "# Date range validation\n",
    "date_min = df['transaction_date'].min()\n",
    "date_max = df['transaction_date'].max()\n",
    "expected_min = pd.to_datetime(BUSINESS_RULES[\"date_range\"][0])\n",
    "expected_max = pd.to_datetime(BUSINESS_RULES[\"date_range\"][1])\n",
    "\n",
    "date_valid = (date_min >= expected_min) and (date_max <= expected_max)\n",
    "validation_results['date_range_valid'] = date_valid\n",
    "print(f\"   üìÖ Date range: {date_min.date()} to {date_max.date()} {'‚úÖ' if date_valid else '‚ùå'}\")\n",
    "\n",
    "# Quantity validation\n",
    "qty_min = df['quantity'].min()\n",
    "qty_max = df['quantity'].max()\n",
    "qty_valid = (qty_min >= BUSINESS_RULES[\"quantity_range\"][0])\n",
    "validation_results['quantity_valid'] = qty_valid\n",
    "print(f\"   üì¶ Quantity range: {qty_min} to {qty_max} {'‚úÖ' if qty_valid else '‚ùå'}\")\n",
    "\n",
    "# Amount validation\n",
    "amt_min = df['total_amount'].min()\n",
    "amt_max = df['total_amount'].max()\n",
    "amt_valid = (amt_min >= BUSINESS_RULES[\"amount_range\"][0]) and (amt_max <= BUSINESS_RULES[\"amount_range\"][1])\n",
    "validation_results['amount_valid'] = amt_valid\n",
    "print(f\"   üí∞ Amount range: ${amt_min:.2f} to ${amt_max:,.2f} {'‚úÖ' if amt_valid else '‚ùå'}\")\n",
    "\n",
    "# Region validation\n",
    "unique_regions = set(df['region'].unique())\n",
    "expected_regions = set(BUSINESS_RULES[\"valid_regions\"])\n",
    "regions_valid = unique_regions.issubset(expected_regions)\n",
    "validation_results['regions_valid'] = regions_valid\n",
    "print(f\"   üåç Regions: {len(unique_regions)} unique {'‚úÖ' if regions_valid else '‚ùå'}\")\n",
    "\n",
    "# Duplicate validation\n",
    "duplicates_remaining = df.duplicated(subset=['transaction_id']).sum()\n",
    "duplicates_valid = (duplicates_remaining == 0)\n",
    "validation_results['no_duplicates'] = duplicates_valid\n",
    "print(f\"   üîÑ Duplicates: {duplicates_remaining} remaining {'‚úÖ' if duplicates_valid else '‚ùå'}\")\n",
    "\n",
    "# FINAL QUALITY SCORE\n",
    "print(f\"\\nüèÜ FINAL QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "missing_customers_final = df['customer_id'].isnull().sum()\n",
    "total_records_final = len(df)\n",
    "\n",
    "# Only missing customer IDs remain as \"issues\" (business decision to keep)\n",
    "remaining_issues = missing_customers_final\n",
    "final_quality_score = max(0, 100 - (remaining_issues / total_records_final * 100))\n",
    "\n",
    "print(f\"   Total records: {total_records_final:,}\")\n",
    "print(f\"   Remaining issues: {remaining_issues:,} (missing customer IDs)\")\n",
    "print(f\"   Final quality score: {final_quality_score:.1f}%\")\n",
    "print(f\"   Quality improvement: {final_quality_score - 96.6:.1f} percentage points\")\n",
    "\n",
    "# EXPORT CLEANED DATA\n",
    "print(f\"\\nüíæ EXPORTING CLEANED DATA\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Remove the flag column before export (internal use only)\n",
    "export_df = df.drop(columns=['missing_customer_flag'], errors='ignore')\n",
    "\n",
    "# Export to processed folder\n",
    "output_file = DATA_PATHS[\"processed\"] + \"sales_cleaned.csv\"\n",
    "export_df.to_csv(output_file, index=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "print(f\"   ‚úÖ Exported: {output_file}\")\n",
    "print(f\"   üìä Records: {len(export_df):,}\")\n",
    "print(f\"   üíæ File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# CREATE CLEANING SUMMARY REPORT\n",
    "cleaning_summary = {\n",
    "    'cleaning_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_records': int(len(sales_df)),  # Convert to regular int\n",
    "    'final_records': int(len(export_df)),\n",
    "    'data_retention_pct': round((len(export_df) / len(sales_df)) * 100, 1),\n",
    "    'quality_improvement': round(final_quality_score - 96.6, 1),\n",
    "    'final_quality_score': round(final_quality_score, 1),\n",
    "    'issues_resolved': {\n",
    "        'duplicates_removed': int(cleaning_log.get('duplicates_removed', 0)),\n",
    "        'future_dates_corrected': int(cleaning_log.get('future_dates_corrected', 0)),\n",
    "        'regions_standardized': int(cleaning_log.get('regions_standardized', 0)),\n",
    "        'outliers_capped': int(cleaning_log.get('outliers_capped', 0)),\n",
    "        'negative_quantities_fixed': int(negative_count)\n",
    "    },\n",
    "    'validation_results': {k: bool(v) for k, v in validation_results.items()}  # Convert to regular bool\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_file = DATA_PATHS[\"processed\"] + \"cleaning_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(cleaning_summary, f, indent=2)\n",
    "\n",
    "print(f\"   ‚úÖ Cleaning summary saved: {summary_file}\")\n",
    "\n",
    "print(f\"\\nüéâ DATA CLEANING FRAMEWORK COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Professional-grade data quality framework implemented\")\n",
    "print(f\"‚úÖ {final_quality_score:.1f}% quality score achieved\")\n",
    "print(f\"‚úÖ Framework ready for reuse on any project\")\n",
    "print(f\"‚úÖ Clean data ready for ML/AI components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb8608-a233-4715-b413-75f4f7b91d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
