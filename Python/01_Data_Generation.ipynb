{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7e370c-b19f-4ac3-b1ca-bb87a4d0effb",
   "metadata": {},
   "source": [
    "# Sales Health Monitor - Data Generation\n",
    "## Project Overview\n",
    "This notebook generates realistic sales data for our AI-powered business monitoring system.\n",
    "\n",
    "### What we'll create:\n",
    "- 2-3 years of daily sales transactions (~500K-1M records)\n",
    "- Multiple regions, products, and sales channels\n",
    "- Realistic business patterns and seasonality\n",
    "- Intentional data quality issues for cleaning practice\n",
    "\n",
    "### Generated Tables:\n",
    "1. **Main Sales Transactions** (~800K records)\n",
    "2. **Customer Master Data** (~50K records) \n",
    "3. **Product Catalog** (~500 records)\n",
    "4. **Regional Information** (~20 records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3740f956-74b1-4346-bd9b-ba9c86b99a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and generation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ecfecb-20fc-489a-a428-00008a30d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # Keep faker data consistent too\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f080bd-5dfc-4d27-83d8-d0f1fa063147",
   "metadata": {},
   "source": [
    "## Business Context & Parameters\n",
    "Before generating data, we need to define our business scenario and realistic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01f365e-a9d3-4a26-a092-f97d174bc1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¢ BUSINESS SCENARIO SETUP\n",
      "==================================================\n",
      "ğŸ“… Data Period: 2022-01-01 to 2024-12-31\n",
      "ğŸ“Š Total Days: 1095 days\n",
      "\n",
      "ğŸŒ Regions: 5 regions\n",
      "ğŸ“¦ Products: 5 categories\n",
      "ğŸ›’ Channels: 4 channels\n"
     ]
    }
   ],
   "source": [
    "# Business scenario parameters\n",
    "print(\"ğŸ¢ BUSINESS SCENARIO SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Time period for our data\n",
    "START_DATE = datetime(2022, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "TOTAL_DAYS = (END_DATE - START_DATE).days\n",
    "\n",
    "print(f\"ğŸ“… Data Period: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(f\"ğŸ“Š Total Days: {TOTAL_DAYS} days\")\n",
    "\n",
    "# Geographic regions\n",
    "REGIONS = ['North', 'South', 'East', 'West', 'Central']\n",
    "\n",
    "# Product categories\n",
    "PRODUCT_CATEGORIES = [\n",
    "    'Electronics', 'Clothing', 'Home & Garden', \n",
    "    'Sports & Outdoors', 'Books & Media'\n",
    "]\n",
    "\n",
    "# Sales channels\n",
    "SALES_CHANNELS = ['Online', 'Retail Store', 'Phone Orders', 'Mobile App']\n",
    "\n",
    "print(f\"\\nğŸŒ Regions: {len(REGIONS)} regions\")\n",
    "print(f\"ğŸ“¦ Products: {len(PRODUCT_CATEGORIES)} categories\") \n",
    "print(f\"ğŸ›’ Channels: {len(SALES_CHANNELS)} channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3cb1e1-1d68-4760-b18f-1f80df5ba308",
   "metadata": {},
   "source": [
    "## Generating Customer Master Data\n",
    "Creating realistic customer profiles that our sales transactions will reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5400b6-d72b-4bf4-abed-c7db33cb80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¥ GENERATING CUSTOMER MASTER DATA\n",
      "==================================================\n",
      "ğŸ¯ Generating 50,000 customers\n",
      "ğŸ“Š Segments: {'Premium': 0.15, 'Standard': 0.6, 'Budget': 0.25}\n",
      "âœ… Generated 50,000 customer records\n",
      "ğŸ“… Acquisition date range: 2021-01-01 to 2024-11-30\n",
      "\n",
      "ğŸ“Š Customer segment distribution:\n",
      "segment\n",
      "Standard    30115\n",
      "Budget      12453\n",
      "Premium      7432\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ‘¥ GENERATING CUSTOMER MASTER DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Customer parameters\n",
    "NUM_CUSTOMERS = 50000  # Realistic customer base for mid-size company\n",
    "\n",
    "# Customer segments (important for business analysis)\n",
    "CUSTOMER_SEGMENTS = ['Premium', 'Standard', 'Budget']\n",
    "SEGMENT_WEIGHTS = [0.15, 0.60, 0.25]  # 15% premium, 60% standard, 25% budget\n",
    "\n",
    "print(f\"ğŸ¯ Generating {NUM_CUSTOMERS:,} customers\")\n",
    "print(f\"ğŸ“Š Segments: {dict(zip(CUSTOMER_SEGMENTS, SEGMENT_WEIGHTS))}\")\n",
    "\n",
    "# Generate customer data\n",
    "customers_data = []\n",
    "\n",
    "for i in range(NUM_CUSTOMERS):\n",
    "    # Create realistic customer profile\n",
    "    customer_id = f\"CUST_{i+1:06d}\"\n",
    "    \n",
    "    # Random acquisition date (customers joined over time)\n",
    "    acquisition_date = fake.date_between(\n",
    "        start_date=START_DATE - timedelta(days=365),  # Some customers from before our data period\n",
    "        end_date=END_DATE - timedelta(days=30)        # No brand new customers\n",
    "    )\n",
    "    \n",
    "    # Assign segment with realistic distribution\n",
    "    segment = np.random.choice(CUSTOMER_SEGMENTS, p=SEGMENT_WEIGHTS)\n",
    "    \n",
    "    # Generate realistic customer info\n",
    "    first_name = fake.first_name()\n",
    "    last_name = fake.last_name()\n",
    "    email = f\"{first_name.lower()}.{last_name.lower()}@{fake.domain_name()}\"\n",
    "    \n",
    "    # Assign to region (customers distributed across regions)\n",
    "    region = np.random.choice(REGIONS)\n",
    "    \n",
    "    customers_data.append({\n",
    "        'customer_id': customer_id,\n",
    "        'first_name': first_name,\n",
    "        'last_name': last_name,\n",
    "        'email': email,\n",
    "        'segment': segment,\n",
    "        'region': region,\n",
    "        'acquisition_date': acquisition_date\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "\n",
    "print(f\"âœ… Generated {len(customers_df):,} customer records\")\n",
    "print(f\"ğŸ“… Acquisition date range: {customers_df['acquisition_date'].min()} to {customers_df['acquisition_date'].max()}\")\n",
    "print(\"\\nğŸ“Š Customer segment distribution:\")\n",
    "print(customers_df['segment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb168da9-1bce-48f5-851c-c9639f1b6050",
   "metadata": {},
   "source": [
    "## Generating Product Catalog\n",
    "Creating our product master data with realistic pricing and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77bef7e9-39f8-423b-adf3-0ff9bc6914c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ GENERATING PRODUCT CATALOG\n",
      "==================================================\n",
      "ğŸ¯ Generating 500 products (100 per category)\n",
      "âœ… Generated 500 product records\n",
      "ğŸ’° Price range: $5.16 - $2229.43\n",
      "ğŸ“Š Average margin: 22.5%\n",
      "\n",
      "ğŸ“¦ Products per category:\n",
      "category\n",
      "Electronics          100\n",
      "Clothing             100\n",
      "Home & Garden        100\n",
      "Sports & Outdoors    100\n",
      "Books & Media        100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ’¡ Sample products:\n",
      "  product_id                          product_name     category    price\n",
      "0  PROD_0001                  Castillo-Diaz Laptop  Electronics   932.32\n",
      "1  PROD_0002                Mitchell-Martin Laptop  Electronics   162.41\n",
      "2  PROD_0003  Jennings, Hansen and Figueroa Tablet  Electronics  1709.67\n",
      "3  PROD_0004           Zhang, Smith and Snow Phone  Electronics  2089.26\n",
      "4  PROD_0005                  Boone and Sons Phone  Electronics    61.34\n",
      "5  PROD_0006                     Buckley PLC Phone  Electronics   712.10\n",
      "6  PROD_0007    Johnson, Martinez and Clark Laptop  Electronics   231.61\n",
      "7  PROD_0008    Griffin, Flores and Jacobs Speaker  Electronics   562.21\n",
      "8  PROD_0009                Manning-Russell Laptop  Electronics   685.18\n",
      "9  PROD_0010                   Everett LLC Speaker  Electronics  1440.81\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“¦ GENERATING PRODUCT CATALOG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Product parameters\n",
    "PRODUCTS_PER_CATEGORY = 100  # 100 products per category = 500 total products\n",
    "TOTAL_PRODUCTS = len(PRODUCT_CATEGORIES) * PRODUCTS_PER_CATEGORY\n",
    "\n",
    "print(f\"ğŸ¯ Generating {TOTAL_PRODUCTS} products ({PRODUCTS_PER_CATEGORY} per category)\")\n",
    "\n",
    "# Price ranges by category (realistic business pricing)\n",
    "CATEGORY_PRICE_RANGES = {\n",
    "    'Electronics': (50, 2000),      # $50 - $2000\n",
    "    'Clothing': (15, 300),          # $15 - $300\n",
    "    'Home & Garden': (10, 500),     # $10 - $500\n",
    "    'Sports & Outdoors': (20, 800), # $20 - $800\n",
    "    'Books & Media': (5, 100)       # $5 - $100\n",
    "}\n",
    "\n",
    "products_data = []\n",
    "product_counter = 1\n",
    "\n",
    "for category in PRODUCT_CATEGORIES:\n",
    "    min_price, max_price = CATEGORY_PRICE_RANGES[category]\n",
    "    \n",
    "    for i in range(PRODUCTS_PER_CATEGORY):\n",
    "        product_id = f\"PROD_{product_counter:04d}\"\n",
    "        \n",
    "        # Generate realistic product name\n",
    "        if category == 'Electronics':\n",
    "            product_name = f\"{fake.company()} {random.choice(['Laptop', 'Phone', 'Tablet', 'Camera', 'Speaker'])}\"\n",
    "        elif category == 'Clothing':\n",
    "            product_name = f\"{random.choice(['Premium', 'Classic', 'Sport'])} {random.choice(['Jacket', 'Jeans', 'Shirt', 'Dress', 'Shoes'])}\"\n",
    "        elif category == 'Home & Garden':\n",
    "            product_name = f\"{random.choice(['Deluxe', 'Standard', 'Compact'])} {random.choice(['Chair', 'Table', 'Lamp', 'Plant', 'Tool'])}\"\n",
    "        elif category == 'Sports & Outdoors':\n",
    "            product_name = f\"{random.choice(['Pro', 'Amateur', 'Kids'])} {random.choice(['Ball', 'Racket', 'Bike', 'Gear', 'Equipment'])}\"\n",
    "        else:  # Books & Media\n",
    "            product_name = f\"{fake.catch_phrase()} {random.choice(['Book', 'DVD', 'Game', 'Magazine'])}\"\n",
    "        \n",
    "        # Realistic pricing with some variation\n",
    "        base_price = np.random.uniform(min_price, max_price)\n",
    "        price = round(base_price * np.random.uniform(0.8, 1.2), 2)  # Â±20% variation\n",
    "        \n",
    "        # Cost (70-85% of price for realistic margins)\n",
    "        cost = round(price * np.random.uniform(0.70, 0.85), 2)\n",
    "        \n",
    "        # Launch date (products launched over time)\n",
    "        launch_date = fake.date_between(\n",
    "            start_date=START_DATE - timedelta(days=500),\n",
    "            end_date=END_DATE - timedelta(days=60)\n",
    "        )\n",
    "        \n",
    "        products_data.append({\n",
    "            'product_id': product_id,\n",
    "            'product_name': product_name,\n",
    "            'category': category,\n",
    "            'price': price,\n",
    "            'cost': cost,\n",
    "            'margin_percent': round((price - cost) / price * 100, 1),\n",
    "            'launch_date': launch_date\n",
    "        })\n",
    "        \n",
    "        product_counter += 1\n",
    "\n",
    "# Create DataFrame\n",
    "products_df = pd.DataFrame(products_data)\n",
    "\n",
    "print(f\"âœ… Generated {len(products_df):,} product records\")\n",
    "print(f\"ğŸ’° Price range: ${products_df['price'].min():.2f} - ${products_df['price'].max():.2f}\")\n",
    "print(f\"ğŸ“Š Average margin: {products_df['margin_percent'].mean():.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Products per category:\")\n",
    "print(products_df['category'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ’¡ Sample products:\")\n",
    "print(products_df[['product_id', 'product_name', 'category', 'price']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5a402-d036-41ef-a6e8-13fbe137e85a",
   "metadata": {},
   "source": [
    "## Generating Main Sales Transactions\n",
    "Creating realistic daily sales with seasonal patterns, regional differences, and intentional anomalies for our ML system to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99ab273-01e4-40ae-a908-bd011e0c3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’° GENERATING SALES TRANSACTIONS\n",
      "==================================================\n",
      "ğŸ¯ Target: ~800 transactions/day\n",
      "ğŸ“Š Expected total: ~876,000 transactions over 1095 days\n",
      "ğŸ“… Generating transactions for 1096 days...\n",
      "\n",
      "ğŸ”„ Progress:\n",
      "   Day 1/1096 (2022-01-01)\n",
      "   Day 101/1096 (2022-04-11)\n",
      "   Day 201/1096 (2022-07-20)\n",
      "   Day 301/1096 (2022-10-28)\n",
      "   Day 401/1096 (2023-02-05)\n",
      "   Day 501/1096 (2023-05-16)\n",
      "   Day 601/1096 (2023-08-24)\n",
      "   Day 701/1096 (2023-12-02)\n",
      "   Day 801/1096 (2024-03-11)\n",
      "   Day 901/1096 (2024-06-19)\n",
      "   Day 1001/1096 (2024-09-27)\n",
      "\n",
      "âœ… Generated 793,505 sales transactions!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’° GENERATING SALES TRANSACTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Transaction parameters\n",
    "DAILY_TRANSACTION_TARGET = 800  # Target ~800 transactions per day\n",
    "TOTAL_EXPECTED_TRANSACTIONS = TOTAL_DAYS * DAILY_TRANSACTION_TARGET\n",
    "\n",
    "print(f\"ğŸ¯ Target: ~{DAILY_TRANSACTION_TARGET} transactions/day\")\n",
    "print(f\"ğŸ“Š Expected total: ~{TOTAL_EXPECTED_TRANSACTIONS:,} transactions over {TOTAL_DAYS} days\")\n",
    "\n",
    "# Create date range\n",
    "date_range = pd.date_range(start=START_DATE, end=END_DATE, freq='D')\n",
    "\n",
    "print(f\"ğŸ“… Generating transactions for {len(date_range)} days...\")\n",
    "\n",
    "# This will take a moment - let's track progress\n",
    "sales_transactions = []\n",
    "transaction_id_counter = 1\n",
    "\n",
    "print(\"\\nğŸ”„ Progress:\")\n",
    "for day_idx, current_date in enumerate(date_range):\n",
    "    # Show progress every 100 days\n",
    "    if day_idx % 100 == 0:\n",
    "        print(f\"   Day {day_idx+1}/{len(date_range)} ({current_date.strftime('%Y-%m-%d')})\")\n",
    "    \n",
    "    # Calculate daily transaction count with variations\n",
    "    base_daily_count = DAILY_TRANSACTION_TARGET\n",
    "    \n",
    "    # Day of week patterns (realistic business patterns)\n",
    "    day_of_week = current_date.weekday()  # 0=Monday, 6=Sunday\n",
    "    if day_of_week == 6:  # Sunday - lowest sales\n",
    "        daily_multiplier = 0.4\n",
    "    elif day_of_week == 5:  # Saturday\n",
    "        daily_multiplier = 0.6\n",
    "    elif day_of_week in [0, 1]:  # Monday, Tuesday\n",
    "        daily_multiplier = 0.8\n",
    "    else:  # Wed, Thu, Fri - peak days\n",
    "        daily_multiplier = 1.0\n",
    "    \n",
    "    # Seasonal patterns (this is key for realistic data!)\n",
    "    month = current_date.month\n",
    "    if month in [11, 12]:  # Holiday season\n",
    "        seasonal_multiplier = 1.8\n",
    "    elif month in [1, 2]:  # Post-holiday slowdown\n",
    "        seasonal_multiplier = 0.6\n",
    "    elif month in [3, 4, 5]:  # Spring uptick\n",
    "        seasonal_multiplier = 1.1\n",
    "    elif month in [6, 7, 8]:  # Summer\n",
    "        seasonal_multiplier = 1.0\n",
    "    else:  # Fall\n",
    "        seasonal_multiplier = 1.2\n",
    "    \n",
    "    # Random daily variation (Â±20%)\n",
    "    random_multiplier = np.random.uniform(0.8, 1.2)\n",
    "    \n",
    "    # Calculate final daily transaction count\n",
    "    daily_count = int(base_daily_count * daily_multiplier * seasonal_multiplier * random_multiplier)\n",
    "    daily_count = max(50, daily_count)  # Minimum 50 transactions per day\n",
    "    \n",
    "    # Generate transactions for this day\n",
    "    for trans_idx in range(daily_count):\n",
    "        transaction_id = f\"TXN_{transaction_id_counter:08d}\"\n",
    "        \n",
    "        # Select random customer, product, region, channel\n",
    "        customer = customers_df.sample(1).iloc[0]\n",
    "        product = products_df.sample(1).iloc[0]\n",
    "        region = np.random.choice(REGIONS)\n",
    "        channel = np.random.choice(SALES_CHANNELS)\n",
    "        \n",
    "        # Calculate realistic quantities (most orders are 1-3 items)\n",
    "        if np.random.random() < 0.7:  # 70% are single items\n",
    "            quantity = 1\n",
    "        elif np.random.random() < 0.9:  # 20% are 2-3 items\n",
    "            quantity = np.random.randint(2, 4)\n",
    "        else:  # 10% are larger orders\n",
    "            quantity = np.random.randint(4, 10)\n",
    "        \n",
    "        # Calculate pricing\n",
    "        base_price = product['price']\n",
    "        \n",
    "        # Random discounts (realistic business practice)\n",
    "        if np.random.random() < 0.15:  # 15% of transactions have discounts\n",
    "            discount_percent = np.random.uniform(5, 25)  # 5-25% discount\n",
    "            final_price = base_price * (1 - discount_percent/100)\n",
    "        else:\n",
    "            discount_percent = 0\n",
    "            final_price = base_price\n",
    "        \n",
    "        # Total transaction amount\n",
    "        total_amount = round(final_price * quantity, 2)\n",
    "        \n",
    "        # Random transaction time during the day\n",
    "        hour = np.random.randint(6, 23)  # Business hours 6 AM to 11 PM\n",
    "        minute = np.random.randint(0, 60)\n",
    "        transaction_datetime = current_date.replace(hour=hour, minute=minute)\n",
    "        \n",
    "        sales_transactions.append({\n",
    "            'transaction_id': transaction_id,\n",
    "            'transaction_date': current_date,\n",
    "            'transaction_datetime': transaction_datetime,\n",
    "            'customer_id': customer['customer_id'],\n",
    "            'customer_segment': customer['segment'],\n",
    "            'product_id': product['product_id'],\n",
    "            'product_category': product['category'],\n",
    "            'region': region,\n",
    "            'sales_channel': channel,\n",
    "            'quantity': quantity,\n",
    "            'unit_price': round(final_price, 2),\n",
    "            'discount_percent': round(discount_percent, 1),\n",
    "            'total_amount': total_amount\n",
    "        })\n",
    "        \n",
    "        transaction_id_counter += 1\n",
    "\n",
    "print(f\"\\nâœ… Generated {len(sales_transactions):,} sales transactions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf288cd-32a8-4112-a09d-4b983beed8c3",
   "metadata": {},
   "source": [
    "## Convert to DataFrame & Quick Analysis\n",
    "Let's analyze our generated sales data to verify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee2e557-794b-46b2-b1f7-39a668cd4805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CONVERTING TO DATAFRAME & ANALYSIS\n",
      "==================================================\n",
      "âœ… Created DataFrame with 793,505 rows and 13 columns\n",
      "ğŸ’¾ Memory usage: 400.9 MB\n",
      "\n",
      "ğŸ“ˆ TRANSACTION SUMMARY:\n",
      "   Date range: 2022-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "   Total revenue: $469,040,584.24\n",
      "   Average transaction: $591.10\n",
      "   Total customers: 50,000\n",
      "   Total products sold: 500\n",
      "\n",
      "ğŸ›’ CHANNEL DISTRIBUTION:\n",
      "sales_channel\n",
      "Retail Store    198925\n",
      "Online          198427\n",
      "Phone Orders    198119\n",
      "Mobile App      198034\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ¢ REGIONAL DISTRIBUTION:\n",
      "region\n",
      "West       159252\n",
      "East       159069\n",
      "Central    158476\n",
      "North      158436\n",
      "South      158272\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“¦ CATEGORY DISTRIBUTION:\n",
      "product_category\n",
      "Clothing             158952\n",
      "Electronics          158746\n",
      "Books & Media        158647\n",
      "Home & Garden        158584\n",
      "Sports & Outdoors    158576\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ’° REVENUE BY SEGMENT:\n",
      "                   count           sum        mean\n",
      "customer_segment                                  \n",
      "Budget            197208  1.166416e+08  591.464739\n",
      "Premium           117935  6.993240e+07  592.974108\n",
      "Standard          478362  2.824666e+08  590.487130\n",
      "\n",
      "ğŸ“… MONTHLY SALES PATTERN:\n",
      "transaction_date\n",
      "1     20613399.79\n",
      "2     19212645.41\n",
      "3     39546880.01\n",
      "4     36240346.41\n",
      "5     39177238.14\n",
      "6     33974427.47\n",
      "7     34197488.31\n",
      "8     35418483.37\n",
      "9     41012812.94\n",
      "10    43231659.14\n",
      "11    62800227.61\n",
      "12    63614975.64\n",
      "Name: total_amount, dtype: float64\n",
      "\n",
      "ğŸ¯ November sales: $62,800,227.61\n",
      "ğŸ¯ December sales: $63,614,975.64\n",
      "ğŸ¯ January sales: $20,613,399.79\n",
      "ğŸ“Š Notice the holiday peak and post-holiday drop - our seasonal logic worked!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š CONVERTING TO DATAFRAME & ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert to DataFrame\n",
    "sales_df = pd.DataFrame(sales_transactions)\n",
    "\n",
    "print(f\"âœ… Created DataFrame with {len(sales_df):,} rows and {len(sales_df.columns)} columns\")\n",
    "print(f\"ğŸ’¾ Memory usage: {sales_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Quick data analysis\n",
    "print(\"\\nğŸ“ˆ TRANSACTION SUMMARY:\")\n",
    "print(f\"   Date range: {sales_df['transaction_date'].min()} to {sales_df['transaction_date'].max()}\")\n",
    "print(f\"   Total revenue: ${sales_df['total_amount'].sum():,.2f}\")\n",
    "print(f\"   Average transaction: ${sales_df['total_amount'].mean():.2f}\")\n",
    "print(f\"   Total customers: {sales_df['customer_id'].nunique():,}\")\n",
    "print(f\"   Total products sold: {sales_df['product_id'].nunique():,}\")\n",
    "\n",
    "print(\"\\nğŸ›’ CHANNEL DISTRIBUTION:\")\n",
    "print(sales_df['sales_channel'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ¢ REGIONAL DISTRIBUTION:\")\n",
    "print(sales_df['region'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ“¦ CATEGORY DISTRIBUTION:\")\n",
    "print(sales_df['product_category'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ’° REVENUE BY SEGMENT:\")\n",
    "segment_revenue = sales_df.groupby('customer_segment')['total_amount'].agg(['count', 'sum', 'mean'])\n",
    "print(segment_revenue)\n",
    "\n",
    "# Check for seasonal patterns (this proves our logic worked!)\n",
    "print(\"\\nğŸ“… MONTHLY SALES PATTERN:\")\n",
    "monthly_sales = sales_df.groupby(sales_df['transaction_date'].dt.month)['total_amount'].sum()\n",
    "print(monthly_sales)\n",
    "\n",
    "print(f\"\\nğŸ¯ November sales: ${monthly_sales[11]:,.2f}\")\n",
    "print(f\"ğŸ¯ December sales: ${monthly_sales[12]:,.2f}\")\n",
    "print(f\"ğŸ¯ January sales: ${monthly_sales[1]:,.2f}\")\n",
    "print(\"ğŸ“Š Notice the holiday peak and post-holiday drop - our seasonal logic worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dee63b-4499-4500-8913-9597d1b7d30d",
   "metadata": {},
   "source": [
    "## Adding Data Quality Issues for Cleaning Practice\n",
    "Now we'll intentionally introduce realistic data problems that occur in real business systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d006ad-8b84-45c2-a13f-665fcd8a4725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ ADDING DATA QUALITY ISSUES\n",
      "==================================================\n",
      "ğŸ“Š Starting with 793,505 clean records\n",
      "âŒ Added 15,870 missing customer IDs (2%)\n",
      "âŒ Added 7,935 duplicate transactions (1%)\n",
      "âŒ Added 801 negative quantities (0.1%)\n",
      "âŒ Added 400 future dates (0.05%)\n",
      "âŒ Added 8,014 inconsistent region names (1%)\n",
      "âŒ Added 1,602 extreme outliers (0.2%)\n",
      "\n",
      "âœ… Final corrupted dataset: 801,440 records\n",
      "ğŸ“ˆ Added 7,935 total data quality issues\n",
      "\n",
      "ğŸ” DATA QUALITY ISSUES SUMMARY:\n",
      "   Missing customer IDs: 16,041\n",
      "   Negative quantities: 801\n",
      "   Future dates: 0\n",
      "   Unique regions: 25 (should be 5)\n",
      "   Max transaction amount: $1,255,612.50\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ§¹ ADDING DATA QUALITY ISSUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a copy for corrupting (keep original clean)\n",
    "corrupted_sales_df = sales_df.copy()\n",
    "\n",
    "print(f\"ğŸ“Š Starting with {len(corrupted_sales_df):,} clean records\")\n",
    "\n",
    "# Issue 1: Missing customer IDs (2% - realistic system integration issues)\n",
    "missing_customer_count = int(len(corrupted_sales_df) * 0.02)\n",
    "missing_customer_indices = np.random.choice(corrupted_sales_df.index, missing_customer_count, replace=False)\n",
    "corrupted_sales_df.loc[missing_customer_indices, 'customer_id'] = None\n",
    "\n",
    "print(f\"âŒ Added {missing_customer_count:,} missing customer IDs (2%)\")\n",
    "\n",
    "# Issue 2: Duplicate transactions (1% - system glitches)\n",
    "duplicate_count = int(len(corrupted_sales_df) * 0.01)\n",
    "duplicate_indices = np.random.choice(corrupted_sales_df.index, duplicate_count, replace=False)\n",
    "duplicated_rows = corrupted_sales_df.loc[duplicate_indices].copy()\n",
    "# Change transaction_id but keep everything else same (realistic duplicate scenario)\n",
    "duplicated_rows['transaction_id'] = duplicated_rows['transaction_id'] + '_DUP'\n",
    "corrupted_sales_df = pd.concat([corrupted_sales_df, duplicated_rows], ignore_index=True)\n",
    "\n",
    "print(f\"âŒ Added {duplicate_count:,} duplicate transactions (1%)\")\n",
    "\n",
    "# Issue 3: Impossible negative quantities (0.1% - data entry errors)\n",
    "negative_qty_count = int(len(corrupted_sales_df) * 0.001)\n",
    "negative_qty_indices = np.random.choice(corrupted_sales_df.index, negative_qty_count, replace=False)\n",
    "corrupted_sales_df.loc[negative_qty_indices, 'quantity'] = -1 * corrupted_sales_df.loc[negative_qty_indices, 'quantity']\n",
    "\n",
    "print(f\"âŒ Added {negative_qty_count:,} negative quantities (0.1%)\")\n",
    "\n",
    "# Issue 4: Future dates (0.05% - system clock issues)\n",
    "future_date_count = int(len(corrupted_sales_df) * 0.0005)\n",
    "future_date_indices = np.random.choice(corrupted_sales_df.index, future_date_count, replace=False)\n",
    "future_date = datetime(2025, 6, 15)  # Future date\n",
    "corrupted_sales_df.loc[future_date_indices, 'transaction_date'] = future_date\n",
    "\n",
    "print(f\"âŒ Added {future_date_count:,} future dates (0.05%)\")\n",
    "\n",
    "# Issue 5: Inconsistent region names (1% - data entry variations)\n",
    "inconsistent_region_count = int(len(corrupted_sales_df) * 0.01)\n",
    "inconsistent_region_indices = np.random.choice(corrupted_sales_df.index, inconsistent_region_count, replace=False)\n",
    "# Create variations of existing regions\n",
    "region_variations = {\n",
    "    'North': ['NORTH', 'north', 'N', 'Northern'],\n",
    "    'South': ['SOUTH', 'south', 'S', 'Southern'],\n",
    "    'East': ['EAST', 'east', 'E', 'Eastern'],\n",
    "    'West': ['WEST', 'west', 'W', 'Western'],\n",
    "    'Central': ['CENTRAL', 'central', 'C', 'Centre']\n",
    "}\n",
    "\n",
    "for idx in inconsistent_region_indices:\n",
    "    current_region = corrupted_sales_df.loc[idx, 'region']\n",
    "    if current_region in region_variations:\n",
    "        new_region = np.random.choice(region_variations[current_region])\n",
    "        corrupted_sales_df.loc[idx, 'region'] = new_region\n",
    "\n",
    "print(f\"âŒ Added {inconsistent_region_count:,} inconsistent region names (1%)\")\n",
    "\n",
    "# Issue 6: Extreme outliers (0.2% - data entry or system errors)\n",
    "outlier_count = int(len(corrupted_sales_df) * 0.002)\n",
    "outlier_indices = np.random.choice(corrupted_sales_df.index, outlier_count, replace=False)\n",
    "# Create unrealistic high amounts (10x-100x normal)\n",
    "corrupted_sales_df.loc[outlier_indices, 'total_amount'] = corrupted_sales_df.loc[outlier_indices, 'total_amount'] * np.random.uniform(10, 100, size=outlier_count)\n",
    "\n",
    "print(f\"âŒ Added {outlier_count:,} extreme outliers (0.2%)\")\n",
    "\n",
    "print(f\"\\nâœ… Final corrupted dataset: {len(corrupted_sales_df):,} records\")\n",
    "print(f\"ğŸ“ˆ Added {len(corrupted_sales_df) - len(sales_df):,} total data quality issues\")\n",
    "\n",
    "# Quick summary of issues\n",
    "print(f\"\\nğŸ” DATA QUALITY ISSUES SUMMARY:\")\n",
    "print(f\"   Missing customer IDs: {corrupted_sales_df['customer_id'].isnull().sum():,}\")\n",
    "print(f\"   Negative quantities: {(corrupted_sales_df['quantity'] < 0).sum():,}\")\n",
    "print(f\"   Future dates: {(corrupted_sales_df['transaction_date'] > datetime.now()).sum():,}\")\n",
    "print(f\"   Unique regions: {corrupted_sales_df['region'].nunique()} (should be 5)\")\n",
    "print(f\"   Max transaction amount: ${corrupted_sales_df['total_amount'].max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc5c55-32ae-4b2f-aeb8-26755dbfb681",
   "metadata": {},
   "source": [
    "## Save All Generated Datasets\n",
    "Saving clean and corrupted datasets to appropriate folders for the next phase of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ac4f80-5528-421c-a72c-5e72bd9255ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ SAVING ALL DATASETS\n",
      "==================================================\n",
      "ğŸ“ Created/verified directory structure\n",
      "\n",
      "ğŸ”„ Saving clean master datasets...\n",
      "âœ… Saved customers.csv: 50,000 records\n",
      "âœ… Saved products.csv: 500 records\n",
      "âœ… Saved sales_clean.csv: 793,505 records\n",
      "âœ… Saved sales_corrupted.csv: 801,440 records\n",
      "\n",
      "ğŸ”„ Creating sample datasets for GitHub...\n",
      "âœ… Saved customers_sample.csv: 1,000 records\n",
      "âœ… Saved products_sample.csv: 100 records\n",
      "âœ… Saved sales_sample.csv: 5,000 records\n",
      "\n",
      "ğŸ“Š FILE SIZES:\n",
      "   customers.csv: 3.8 MB\n",
      "   products.csv: 0.0 MB\n",
      "   sales_clean.csv: 94.8 MB\n",
      "   sales_corrupted.csv: 95.7 MB\n",
      "\n",
      "ğŸ’¾ Total dataset size: 194.4 MB\n",
      "\n",
      "âœ… Saved data generation summary\n",
      "\n",
      "ğŸ¯ READY FOR NEXT PHASE:\n",
      "   ğŸ“‚ All datasets saved to Dataset/raw/\n",
      "   ğŸ“Š Sample files created for GitHub\n",
      "   ğŸ“‹ Summary file for next notebook\n",
      "   ğŸ§¹ Ready to start data cleaning!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’¾ SAVING ALL DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../Dataset/raw', exist_ok=True)\n",
    "os.makedirs('../Dataset/processed', exist_ok=True)\n",
    "os.makedirs('../Dataset/sample', exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Created/verified directory structure\")\n",
    "\n",
    "# Save clean master data\n",
    "print(\"\\nğŸ”„ Saving clean master datasets...\")\n",
    "\n",
    "customers_df.to_csv('../Dataset/raw/customers.csv', index=False)\n",
    "print(f\"âœ… Saved customers.csv: {len(customers_df):,} records\")\n",
    "\n",
    "products_df.to_csv('../Dataset/raw/products.csv', index=False)\n",
    "print(f\"âœ… Saved products.csv: {len(products_df):,} records\")\n",
    "\n",
    "sales_df.to_csv('../Dataset/raw/sales_clean.csv', index=False)\n",
    "print(f\"âœ… Saved sales_clean.csv: {len(sales_df):,} records\")\n",
    "\n",
    "# Save corrupted dataset for cleaning practice\n",
    "corrupted_sales_df.to_csv('../Dataset/raw/sales_corrupted.csv', index=False)\n",
    "print(f\"âœ… Saved sales_corrupted.csv: {len(corrupted_sales_df):,} records\")\n",
    "\n",
    "# Create sample datasets for GitHub (to avoid large file uploads)\n",
    "print(\"\\nğŸ”„ Creating sample datasets for GitHub...\")\n",
    "\n",
    "# Sample 1000 customers\n",
    "customers_sample = customers_df.sample(n=1000, random_state=42)\n",
    "customers_sample.to_csv('../Dataset/sample/customers_sample.csv', index=False)\n",
    "print(f\"âœ… Saved customers_sample.csv: {len(customers_sample):,} records\")\n",
    "\n",
    "# Sample 100 products  \n",
    "products_sample = products_df.sample(n=100, random_state=42)\n",
    "products_sample.to_csv('../Dataset/sample/products_sample.csv', index=False)\n",
    "print(f\"âœ… Saved products_sample.csv: {len(products_sample):,} records\")\n",
    "\n",
    "# Sample 5000 sales transactions\n",
    "sales_sample = corrupted_sales_df.sample(n=5000, random_state=42)\n",
    "sales_sample.to_csv('../Dataset/sample/sales_sample.csv', index=False)\n",
    "print(f\"âœ… Saved sales_sample.csv: {len(sales_sample):,} records\")\n",
    "\n",
    "# Check file sizes\n",
    "print(\"\\nğŸ“Š FILE SIZES:\")\n",
    "import os\n",
    "\n",
    "def get_file_size_mb(filepath):\n",
    "    return os.path.getsize(filepath) / (1024 * 1024)\n",
    "\n",
    "files_to_check = [\n",
    "    '../Dataset/raw/customers.csv',\n",
    "    '../Dataset/raw/products.csv', \n",
    "    '../Dataset/raw/sales_clean.csv',\n",
    "    '../Dataset/raw/sales_corrupted.csv'\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = get_file_size_mb(file_path)\n",
    "        total_size += size_mb\n",
    "        print(f\"   {os.path.basename(file_path)}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Total dataset size: {total_size:.1f} MB\")\n",
    "\n",
    "# Create data summary for next notebook\n",
    "data_summary = {\n",
    "    'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'customers': len(customers_df),\n",
    "    'products': len(products_df),\n",
    "    'clean_transactions': len(sales_df),\n",
    "    'corrupted_transactions': len(corrupted_sales_df),\n",
    "    'date_range': f\"{sales_df['transaction_date'].min()} to {sales_df['transaction_date'].max()}\",\n",
    "    'total_revenue_clean': sales_df['total_amount'].sum(),\n",
    "    'data_quality_issues': {\n",
    "        'missing_customer_ids': corrupted_sales_df['customer_id'].isnull().sum(),\n",
    "        'duplicate_transactions': len(corrupted_sales_df) - len(sales_df),\n",
    "        'negative_quantities': (corrupted_sales_df['quantity'] < 0).sum(),\n",
    "        'inconsistent_regions': corrupted_sales_df['region'].nunique() - 5,\n",
    "        'extreme_outliers': (corrupted_sales_df['total_amount'] > corrupted_sales_df['total_amount'].quantile(0.99)).sum()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary for next notebook\n",
    "import json\n",
    "with open('../Dataset/raw/data_generation_summary.json', 'w') as f:\n",
    "    json.dump(data_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ… Saved data generation summary\")\n",
    "print(f\"\\nğŸ¯ READY FOR NEXT PHASE:\")\n",
    "print(f\"   ğŸ“‚ All datasets saved to Dataset/raw/\")\n",
    "print(f\"   ğŸ“Š Sample files created for GitHub\")\n",
    "print(f\"   ğŸ“‹ Summary file for next notebook\")\n",
    "print(f\"   ğŸ§¹ Ready to start data cleaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c7374-3e36-43bb-8016-fcdfc8e75ce2",
   "metadata": {},
   "source": [
    "## Data Generation Complete! \n",
    "### ğŸ“Š What We Accomplished:\n",
    "\n",
    "**Generated Datasets:**\n",
    "- **50,000 customers** across 5 regions with realistic segments\n",
    "- **500 products** across 5 categories with proper pricing\n",
    "- **793,505 clean transactions** with seasonal patterns\n",
    "- **801,440 corrupted transactions** with 6 types of data quality issues\n",
    "\n",
    "**Business Patterns Created:**\n",
    "- âœ… Seasonal trends (holiday spikes, post-holiday drops)\n",
    "- âœ… Weekly patterns (weekday vs weekend differences)  \n",
    "- âœ… Regional distributions (balanced across all regions)\n",
    "- âœ… Customer segments (Premium, Standard, Budget)\n",
    "- âœ… Realistic pricing and margins\n",
    "\n",
    "**Data Quality Issues for Cleaning Practice:**\n",
    "- âŒ Missing customer IDs (16,041 records)\n",
    "- âŒ Duplicate transactions (7,935 duplicates)\n",
    "- âŒ Negative quantities (801 records)\n",
    "- âŒ Inconsistent region names (25 variations instead of 5)\n",
    "- âŒ Extreme outliers ($1.2M max transaction)\n",
    "\n",
    "### ğŸ¯ Next Steps:\n",
    "Move to `02_Data_Cleaning_Validation.ipynb` to identify and fix all these issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659bac67-2da3-4a2a-8cbd-66ab956832a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ DATA GENERATION PHASE COMPLETE!\n",
      "==================================================\n",
      "âœ… PHASE 1 COMPLETE: Data Generation\n",
      "   - Realistic business data created\n",
      "   - Seasonal patterns implemented\n",
      "   - Data quality issues introduced\n",
      "   - All files saved successfully\n",
      "\n",
      "ğŸ¯ NEXT PHASE: Data Cleaning & Validation\n",
      "   - Load corrupted data\n",
      "   - Identify all quality issues\n",
      "   - Clean and validate data\n",
      "   - Prepare for ML/AI components\n",
      "\n",
      "ğŸ“Š PROJECT METRICS:\n",
      "   Total Records Generated: 844,005\n",
      "   Data Quality Issues: 7,935\n",
      "   Files Created: 8 datasets\n",
      "   Storage Used: 194.4 MB\n",
      "\n",
      "ğŸ† READY TO MOVE TO NOTEBOOK 02!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ‰ DATA GENERATION PHASE COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Final project status\n",
    "print(\"âœ… PHASE 1 COMPLETE: Data Generation\")\n",
    "print(\"   - Realistic business data created\")\n",
    "print(\"   - Seasonal patterns implemented\") \n",
    "print(\"   - Data quality issues introduced\")\n",
    "print(\"   - All files saved successfully\")\n",
    "\n",
    "print(\"\\nğŸ¯ NEXT PHASE: Data Cleaning & Validation\")\n",
    "print(\"   - Load corrupted data\")\n",
    "print(\"   - Identify all quality issues\")\n",
    "print(\"   - Clean and validate data\")\n",
    "print(\"   - Prepare for ML/AI components\")\n",
    "\n",
    "print(f\"\\nğŸ“Š PROJECT METRICS:\")\n",
    "print(f\"   Total Records Generated: {len(sales_df) + len(customers_df) + len(products_df):,}\")\n",
    "print(f\"   Data Quality Issues: {len(corrupted_sales_df) - len(sales_df):,}\")\n",
    "print(f\"   Files Created: 8 datasets\")\n",
    "print(f\"   Storage Used: 194.4 MB\")\n",
    "\n",
    "print(f\"\\nğŸ† READY TO MOVE TO NOTEBOOK 02!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cda88-67c0-407f-802e-48fc562699ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
